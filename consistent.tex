% \begin{document}

\usetikzlibrary{arrows.meta} % For double arrows

\chapter{Decentralized Database}

In the era of big-data today, localized instance of relational database is no
longer enough to hold the volume of data for toady's requirement. Distributed
key-value store has been of a key area of interest in the past few decades. 
Offering such as DynamoDB, Cassandra, Azure Cloud are a few examples of what
industry leaders are offering to address the data problem.\\

Service provided by a distributed key-value store is collectively offered by a
cluster of nodes. The nodes independently restart, update, crash, join or leave 
the cluster, while the service remains uninterrupted (though with possibly
reduced service). As the user base scales, the service must scale accordingly.\\

Two of the key design principles in a distributed data are partition and
replication.\\

A replica group (RG) is a group of nodes that maintain the same set of data.
Nodes in a RG often spans multiple availability zone (AZ) to maximize uptime.
In case of a regional value that wipes out an entire AZ, the other nodes in the
RG can still maintain the service albeit at reduced QoS. The nodes in the RG are
kept in sync using consensus protocol such as Raft. Typically, a write is only
considered complete and ack'd to client once it has been recorded by the
majority of nodes in the RG.

Partition is a way to split the keyspace into slices. When the keyspace is
partitioned, a RG is only responsible for a slice of the keyspace. Bandwidth
demand is also amortized across all RGs. Partition is typically done using 
consistent hashing. Different from traditional hashing, consistent hashing
minimizes data movement when nodes join and leave the clusters. Consistent
hashing will be covered in detail in a later part of the chapter.\\

Some of the early distributed database design requires a centralized server for
meta management (eg. ZooKeeper). In this chapter, we will specify a fully
decentralized key-value store. To simplify the specification, we will assume 
each node itself is a functioning RG with associated reliability property (this
is considered a solved problem with Raft).This chapter will focus on system
behavior correctness as RGs join or leave the cluster and associated data
migration.

\section{Consistent Hashing}

Before we dive into design detail, we must first describe consistent hashing.
With a traditional hashing algorithm, changing the size of the hash space
requires data movement of the entire cluster. This is very undesirable.
Consistent hashing was introduced to minimize data movement, where movement is
only required when adding or removing nodes in the affected range. 

In consistent hashing, the hash space is assumed to be a ring, where the largest
hash value plus one wraps around to the hash of 0. Servers in a consistent
hashing cluster take up different ranges in the ring. For a given request, the
client where the request lands by hashing the request first, then walks the ring
clockwise until it finds a server. \\ 

Assume the following example:

\begin{center}
\begin{tikzpicture}[scale=1.5]

    \draw (0,0) circle (1.5cm);

    Draw the nodes on the circle with updated labels
    \foreach \angle/\label in {0/n0, 72/n1, 144/n2, 216/n3, 288/n4} {
        \node[draw, circle, fill=blue!20, minimum size=8mm] at (\angle:1.5cm) (\label) {\label};
    }

    \draw[->, thick, red] (n1) to[out=0, in=90] (n0); % 2 -> 1
    \draw[->, thick, red] (n0) to[out=270, in=0] (n4); % 1 -> 5

\end{tikzpicture}
\end{center}

If the request lands between n1 (exclusive) and n0 (inclusive), the request will
be processed by n0. Similarly, if the request lands between n0 (exclusive) and
n4 (inclusive), the request is to be processed by n4.\\

Assume a case where n4 goes offline: 

\begin{center}
\begin{tikzpicture}[scale=1.5]

    \draw (0,0) circle (1.5cm);

    % Draw the nodes on the circle with updated labels
    \foreach \angle/\label in {0/n0, 72/n1, 144/n2, 216/n3} {
        \node[draw, circle, fill=blue!20, minimum size=8mm] at (\angle:1.5cm) (\label) {\label};
    }

    \draw[->, thick, red] (n1) to[out=0, in=90] (n0); % 2 -> 1
    \draw[->, thick, red] (n0) to[out=270, in=300] (n3); % 1 -> 5

\end{tikzpicture}
\end{center}

In such a case, requests previously processed by n4 will land on n3 instead.
Similarly, if a new node n5 is added:

\begin{center}
\begin{tikzpicture}[scale=1.5]

    % Draw the circle
    \draw (0,0) circle (1.5cm);

    % Draw the nodes on the circle with updated labels
    \foreach \angle/\label in {0/n0, 72/n1, 144/n2, 216/n3, 288/n4, 324/n5} {
        \node[draw, circle, fill=blue!20, minimum size=8mm] at (\angle:1.5cm) (\label) {\label};
    }

    % Draw arrows
    \draw[->, thick, red] (n1) to[out=0, in=90] (n0); % n1 -> n0
    \draw[->, thick, red] (n0) to[out=300, in=0] (n5); % n0 -> n4
    \draw[->, thick, red] (n5) to[out=270, in=0] (n4); % n0 -> n4

\end{tikzpicture}
\end{center}

Part of what n4 used to service will now be serviced by n5.

\section{Gossip Protocol}

Without a centralized metadata controller, the nodes learn about the peers using
gossip protocol. As a new RG enters the cluster, the design relies on gossip
protocol to spread the information. This is a critical part of the design as
will be described later.

\section{Design}

In our design, a RG can be in one of the following states: Offline, Joining,
Online, Leaving. The service starts with a single RG responsible for the entire
hash space. Since this is the epoch RG, it directly transitions into Online
state and can claim any token on the ring. For simplicity, epoch RG always claim
token 0.\\

The design assumes node failure handling are handled within the RG, the
specification will not model RG crash or restart. 

\subsection{Offline}

RG is offline, no impact to cluster.

\subsection{Joining}

By definition, the goal of adding a RG U into the cluster is to reduce the load
on another RG V. Since RG U is a new member of the cluster, it may not have the
latest cluster topology. Once RG U announces its presence via gossip protocol,
it waits for RG V to reach out.\\

When RG V realizes a new RG can share its burden, RG V will coordinate with RG U
to migrate a subset of its data to RG U (the subset RG U will be responsible
for). Once data migration completes: RG U transitions to Online state and start
servicing request in its range, while RG V rejects request to the range RG U has
taken over. This range update is also reflected in both RG U and V's local ring
cache, and communicated during next round of gossip protocol.

\subsection{Online}

RG is \textit{Online} and responds and records requests in its hash range. 

\subsection{Leaving}

Symmetrically, when a RG U is leaving the cluster, a RG V needs to take over the
range and data RG U is currently responsible for. Similarly to join, RG U 
announce its intent to leave, and wait for RG V to reach out and coordinate the
hand off.\\

Note when RG U is still waiting or in the middle of hand-off, it is still
\textit{Online} and must respond to request, until RV V fully takes over.

% Note this is a \textit{graceful} hand-off. The design assumes any failure
% mitigation is implemented within the RG. 


% Since the entire hash space is always covered by the cluster, a RG U joining 
% the cluster needs to coordinate with another RG V to take over part of RG V's 
% hash space. 
% take over a certain hash range needs to coordinate with RG V which is currently 
% responsible for that 
% cluster needs to coordinate with RG V that is currently responsible for the
% range. When RG U joins the cluster, it will announce its presence via gossip
% protocol. Eventually RG V will realize it can volunteer a part of its range to
% RG U, and 


% In this chapter, we will implement a simple distributed key-value store that
% supports horizontal scale-out. The service provider can add servers into the
% cluster dynamically to reduce the load on individual servers. Similarly, the
% service provider can also remove servers dynamically during off hours to
% minimize server costs.\\

% A new server \textit{K} joining the cluster claims a token \textit{T} on the
% ring. Starting from \textit{T}, walk the ring counter-clockwise to find the
% first neighboring token \textit{P}, \textit{K} owns the range of keys from
% \textit{P} (exclusive) to \textit{T} (inclusive).\\

% The design does not include replication or fault recovery to limit the scope.

\section{Spec}

The distributed key-value \textit{Init} is defined below:\\

\begin{tla}
Init ==
    /\ cluster = {}
    /\ global_ring = [kk \in {} |-> ""]
    /\ local_kv = [k \in Nodes |-> {}]
    /\ global_kv = {}
\end{tla}
\begin{tlatex}
\@x{ Init \.{\defeq}}%
\@x{\@s{16.4} \.{\land} cluster \.{=} \{ \}}%
 \@x{\@s{16.4} \.{\land} global\_ring \.{=} [ kk \.{\in} \{ \}
 \.{\mapsto}\@w{} ]}%
 \@x{\@s{16.4} \.{\land} local\_kv \.{=} [ k \.{\in} Nodes \.{\mapsto} \{ \}
 ]}%
\@x{\@s{16.4} \.{\land} global\_kv \.{=} \{ \}}%
\end{tlatex}

\begin{itemize}
    \item \textit{cluster} is a set that includes all the nodes currently in the distributed key-value store.
    \item \textit{global\_ring} is a function that implements key-to-node mapping.
    \item \textit{local\_kv} represents the per node key-value store. For
    simplicity the key and value are assumed to be the same, thus represented
    as a single number.
    \item \textit{global\_kv} represents the system key-value store, used to check consistency.
\end{itemize}

Before we define the \textit{Spec}, we need create a few helper functions:\\
\begin{tla}
ValueToKey(f, v) == 
    CHOOSE only \in {n \in DOMAIN f: f[n] = v}: TRUE

TokensClaimed == 
    DOMAIN global_ring
\end{tla}
\begin{tlatex}
\@x{ ValueToKey ( f ,\, v ) \.{\defeq}}%
 \@x{\@s{16.4} {\CHOOSE} only \.{\in} \{ n \.{\in} {\DOMAIN} f \.{:} f [ n ]
 \.{=} v \} \.{:} {\TRUE}}%
\@pvspace{8.0pt}%
\@x{ TokensClaimed \.{\defeq}}%
\@x{\@s{16.4} {\DOMAIN} global\_ring}%
\end{tlatex}
\\

\textit{ValueToKey} takes the value to a function and returns the corresponding
key. The definition first creates a set (using set comprehension) containing the
key corresponding to the value. However, we want to return the key itself,
not as part of a set. The definition uses \textit{CHOOSE} to remove the set
container. \textit{CHOOSE} always returns the same value in the set. This is
acceptable for a set containing a single value.\\

\textit{TokensClaimed} returns all the tokens taken on the ring. Since
\textit{global\_ring} is a function mapping containing both key and value, to
return keys only as a set, we use the \textit{DOMAIN} keyword.\\

Working with a hash ring often requires \textit{walking} the ring. We will
define a few helper functions below:\\

\begin{tla}
RECURSIVE FindNextToken(_, _)
FindNextToken(ring, k) ==
    IF k \in DOMAIN ring THEN
        k 
    ELSE 
        FindNextToken(ring, (k + 1) % N) 

RECURSIVE FindPrevToken(_, _)
FindPrevToken(ring, k) ==
    IF k \in DOMAIN ring THEN
        k
    ELSE 
        FindPrevToken(ring, (k - 1 + N) % N) 
\end{tla}
\begin{tlatex}
\@x{ {\RECURSIVE} FindNextToken ( \_ ,\, \_ )}%
\@x{ FindNextToken ( ring ,\, k ) \.{\defeq}}%
\@x{\@s{16.4} {\IF} k \.{\in} {\DOMAIN} ring \.{\THEN}}%
\@x{\@s{20.5} k}%
\@x{\@s{16.4} \.{\ELSE}}%
\@x{\@s{32.8} FindNextToken ( ring ,\, ( k \.{+} 1 ) \.{\%} N )}%
\@pvspace{8.0pt}%
\@x{ {\RECURSIVE} FindPrevToken ( \_ ,\, \_ )}%
\@x{ FindPrevToken ( ring ,\, k ) \.{\defeq}}%
\@x{\@s{16.4} {\IF} k \.{\in} {\DOMAIN} ring \.{\THEN}}%
\@x{\@s{20.5} k}%
\@x{\@s{16.4} \.{\ELSE}}%
\@x{\@s{32.8} FindPrevToken ( ring ,\, ( k \.{-} 1 \.{+} N ) \.{\%} N )}%
\end{tlatex}
\\

\textit{FindNextToken} walks the ring clockwise looking for the next token.
Similarly, \textit{FindPrevToken} walks the ring counter-clockwise looking for the
previous token. Walking the ring is implemented recursively by adding or 
subtracting one. Note that in the counter-clockwise case, care is applied to
ensure the wrap-around works correctly.\\

\begin{tla}
DataSet(ring, my_key) == 
    LET 
        prev_key == FindPrevToken(ring, (my_key + N -1) % N)
        pkey_next == (prev_key + 1) % N
    IN 
        IF pkey_next <= my_key THEN
            {k \in pkey_next..my_key: k \in global_kv}
        ELSE 
            {k \in pkey_next..N-1: k \in global_kv} \cup
            {k \in 0..my_key: k \in global_kv}
\end{tla}
\begin{tlatex}
\@x{ DataSet ( ring ,\, my\_key ) \.{\defeq}}%
\@x{\@s{16.4} \.{\LET}}%
 \@x{\@s{32.8} prev\_key \.{\defeq} FindPrevToken ( ring ,\, ( my\_key \.{+} N
 \.{-} 1 ) \.{\%} N )}%
\@x{\@s{32.8} pkey\_next \.{\defeq} ( prev\_key \.{+} 1 ) \.{\%} N}%
\@x{\@s{16.4} \.{\IN}}%
\@x{\@s{32.8} {\IF} pkey\_next \.{\leq} my\_key \.{\THEN}}%
 \@x{\@s{36.89} \{ k \.{\in} pkey\_next \.{\dotdot} my\_key \.{:} k \.{\in}
 global\_kv \}}%
\@x{\@s{32.8} \.{\ELSE}}%
 \@x{\@s{49.19} \{ k \.{\in} pkey\_next \.{\dotdot} N \.{-} 1 \.{:} k \.{\in}
 global\_kv \} \.{\cup}}%
 \@x{\@s{49.19} \{ k \.{\in} 0 \.{\dotdot} my\_key \.{:} k \.{\in} global\_kv
 \}}%
\end{tlatex}
\\

\textit{DataSet} defines all the key-value entries associated with a given
token. Since key and value are always assumed to be the same in this design, the
key-value entries is represented as a set.\\

With the helper functions defined, let us look at the core part of specification:\\
\begin{tla}
Next ==
    \/ \E u \in Nodes:
        /\ u \notin cluster
        /\ Join(u) 
    \/ \E u \in cluster:
        /\ Leave(u) 
    \/ \E u \in cluster:
        /\ \E k \in KeySpace:
            /\ k \notin global_kv
            /\ Write(u, k)
\end{tla}
\begin{tlatex}
\@x{ Next \.{\defeq}}%
\@x{\@s{16.4} \.{\lor} \E\, u \.{\in} Nodes \.{:}}%
\@x{\@s{20.5} \.{\land} u \.{\notin} cluster}%
\@x{\@s{20.5} \.{\land} Join ( u )}%
\@x{\@s{16.4} \.{\lor} \E\, u \.{\in} cluster \.{:}}%
\@x{\@s{20.5} \.{\land} Leave ( u )}%
\@x{\@s{16.4} \.{\lor} \E\, u \.{\in} cluster \.{:}}%
\@x{\@s{20.5} \.{\land} \E\, k \.{\in} KeySpace \.{:}}%
\@x{\@s{24.6} \.{\land} k \.{\notin} global\_kv}%
\@x{\@s{24.6} \.{\land} Write ( u ,\, k )}%
\end{tlatex}
\\

\textit{Spec} allows a node to join the cluster if the node is not already in
the cluster. A node can leave the cluster if it is part of the cluster. Finally,
a client can pick any server and write to the distributed key-value store.\\

Let us look at \textit{Join} first:\\

\begin{tla}
Join(u) == 
    /\ \E key \in KeySpace:
        /\ key \notin TokensClaimed
        /\ global_ring' = [x \in (DOMAIN global_ring) \cup {key} |->
                        IF x = key THEN u ELSE global_ring[x]]
        /\  IF Cardinality(cluster) # 0 THEN
                local_kv' = [local_kv EXCEPT ![u] = DataSet(global_ring', key)]
            ELSE 
                UNCHANGED local_kv
    /\ cluster' = cluster \cup {u}
    /\ UNCHANGED <<global_kv>>
\end{tla}
\begin{tlatex}
\@x{ Join ( u ) \.{\defeq}}%
\@x{ \.{\land} \E\, key \.{\in} KeySpace \.{:}}%
\@x{\@s{4.1} \.{\land} key \.{\notin} TokensClaimed}%
 \@x{\@s{4.1} \.{\land} global\_ring \.{'} \.{=} [ x \.{\in} ( {\DOMAIN}
 global\_ring ) \.{\cup} \{ key \} \.{\mapsto}}%
\@x{\@s{4.1} {\IF} x \.{=} key \.{\THEN} u \.{\ELSE} global\_ring [ x ] ]}%
 \@x{\@s{4.1} \.{\land}\@s{4.1} {\IF} Cardinality ( cluster ) \.{\neq} 0
 \.{\THEN}}%
 \@x{\@s{12.29} local\_kv \.{'} \.{=} [ local\_kv {\EXCEPT} {\bang} [ u ]
 \.{=} DataSet ( global\_ring \.{'} ,\, key ) ]}%
\@x{\@s{8.2} \.{\ELSE}}%
\@x{\@s{24.59} {\UNCHANGED} local\_kv}%
\@x{ \.{\land} cluster \.{'} \.{=} cluster \.{\cup} \{ u \}}%
\@x{ \.{\land} {\UNCHANGED} {\langle} global\_kv {\rangle}}%
\end{tlatex}
\\

Assume node \textit{u} and \textit{v}. When \textit{u} joins the cluster, it
first claims an unclaimed token on the ring. After the new node determines to 
claim token \textit{T}, the node now \textit{owns} the range of data associated
with \textit{T}.\\

Let us now look at the definition for \textit{Leave}:\\

\begin{tla}
Leave(u) == 
    LET 
        k == ValueToKey(global_ring, u)
        key_next == FindNextToken(global_ring, (k + 1) % N)
        owner_next == global_ring[key_next]
        kv1 == [local_kv EXCEPT ![u] = {}]
        kv2 == [kv1 EXCEPT ![owner_next] 
            = kv1[owner_next] \cup DataSet(global_ring, k)]
    IN 
        /\ Cardinality(cluster) > 1
        /\ global_ring' = [n \in DOMAIN global_ring \ {k} |-> global_ring[n]]
        /\ local_kv' = kv2
        /\ cluster' = cluster \ {u}
        /\ UNCHANGED << global_kv>>
\end{tla}
\begin{tlatex}
\@x{ Leave ( u ) \.{\defeq}}%
\@x{\@s{16.4} \.{\LET}}%
\@x{\@s{32.8} k \.{\defeq} ValueToKey ( global\_ring ,\, u )}%
 \@x{\@s{32.8} key\_next \.{\defeq} FindNextToken ( global\_ring ,\, ( k \.{+}
 1 ) \.{\%} N )}%
\@x{\@s{32.8} owner\_next \.{\defeq} global\_ring [ key\_next ]}%
 \@x{\@s{32.8} kv1 \.{\defeq} [ local\_kv {\EXCEPT} {\bang} [ u ] \.{=} \{ \}
 ]}%
\@x{\@s{32.8} kv2 \.{\defeq} [ kv1 {\EXCEPT} {\bang} [ owner\_next ]}%
 \@x{\@s{32.8} \.{=} kv1 [ owner\_next ] \.{\cup} DataSet ( global\_ring ,\, k
 ) ]}%
\@x{\@s{16.4} \.{\IN}}%
\@x{\@s{32.8} \.{\land} Cardinality ( cluster ) \.{>} 1}%
 \@x{\@s{32.8} \.{\land} global\_ring \.{'} \.{=} [ n \.{\in} {\DOMAIN}
 global\_ring \.{\,\backslash\,} \{ k \} \.{\mapsto} global\_ring [ n ] ]}%
\@x{\@s{32.8} \.{\land} local\_kv \.{'} \.{=} kv2}%
 \@x{\@s{32.8} \.{\land} cluster \.{'} \.{=} cluster \.{\,\backslash\,} \{ u
 \}}%
\@x{\@s{32.8} \.{\land} {\UNCHANGED} {\langle} global\_kv {\rangle}}%
\end{tlatex}
\\

Opposite to \textit{Join}: when a node \textit{Leave}, it needs to copy data to
the \textit{next} token. We can find the next token using \textit{FindNextToken},
then copy the leaving nodes to it.\\

Finally, let us look at the definition of \textit{Write}:\\
\begin{tla}
Write(u, k) == 
    LET 
        key == FindNextToken(global_ring, k)
        owner == global_ring[key]
        up == [local_kv EXCEPT ![owner] = local_kv[owner] \cup {k}]
    IN 
        /\ local_kv' = up
        /\ global_kv' = global_kv \cup {k}
        /\ UNCHANGED <<cluster, global_ring>>
\end{tla}
\begin{tlatex}
\@x{ Write ( u ,\, k ) \.{\defeq}}%
\@x{\@s{16.4} \.{\LET}}%
\@x{\@s{32.8} key \.{\defeq} FindNextToken ( global\_ring ,\, k )}%
\@x{\@s{32.8} owner \.{\defeq} global\_ring [ key ]}%
 \@x{\@s{32.8} up \.{\defeq} [ local\_kv {\EXCEPT} {\bang} [ owner ] \.{=}
 local\_kv [ owner ] \.{\cup} \{ k \} ]}%
\@x{\@s{16.4} \.{\IN}}%
\@x{\@s{32.8} \.{\land} local\_kv \.{'} \.{=} up}%
\@x{\@s{32.8} \.{\land} global\_kv \.{'} \.{=} global\_kv \.{\cup} \{ k \}}%
 \@x{\@s{32.8} \.{\land} {\UNCHANGED} {\langle} cluster ,\, global\_ring
 {\rangle}}%
\end{tlatex}
\\

To write a key-value pair, we need to find the first token claimed by a
server by walking the ring clockwise with \textit{FindNextToken}. We then find
the corresponding server for the token and write the key-value pair to the
local KV store.

\section{Safety} 

Under all circumstances, the distributed key-value store contains the 
entire data set. This is described by the following safety property by unionizing
the data of all nodes.\\

\begin{tla}
KVConsistent == 
    /\ UNION {local_kv[n] : n \in Nodes} = global_kv
\end{tla}
\begin{tlatex}
\@x{ KVConsistent \.{\defeq}}%
 \@x{\@s{16.4} \.{\land} {\UNION} \{ local\_kv [ n ] \.{:} n \.{\in} Nodes \}
 \.{=} global\_kv}%
\end{tlatex}
\\

While the distributed key-value contains the entire data set, we also want to
ensure the per-node key-value stores do not overlap. This can
be done with \textit{imply} operator. For any pair of distinct nodes,
their key value should not overlap.\\

\begin{tla}
KVXOR == 
    Cardinality(cluster) > 1 => 
        \A u, v \in cluster: u # v => (local_kv[u] \intersect local_kv[v]) = {}
\end{tla}
\begin{tlatex}
\@x{ KVXOR \.{\defeq}}%
\@x{\@s{16.4} Cardinality ( cluster ) \.{>} 1 \.{\implies}}%
 \@x{\@s{32.8} \A\, u ,\, v \.{\in} cluster \.{:} u \.{\neq} v \.{\implies} (
 local\_kv [ u ] \.{\cap} local\_kv [ v ] ) \.{=} \{ \}}%
\end{tlatex}

\section{Liveness}

Omitted for this chapter.

% \end{document}

