% \begin{document}

\chapter{Simple Scheduler}

Task schedulers are fairly ubiquitous. Every device implements
\textit{something} to manage tasks. Modern desktop or mobile device processes
are non-trivial OS abstractions. Every process maintains its own virtual memory
space for security reasons. Context switching process requires the OS to "clean"
the hardware before running the new process.\newline

For emedded devices such as hard drives or network cards, the security
consideration may be relaxed as users are typically not allowed to run arbitrary
code on the device. Sometimes these products don't have full blown operating
system to save on memory and storage footprint, but still need some sort of
scheduler to manage the tasks.\newline

To solve the c10k \cite{c10k} problem, some languages (eg. Rust) supports
asynchronous programming, allowing the user to enable task switching 
\textit{within} the same process to scale up system throughput.  However,
language like Rust only provides the \textit{language support} for asynchronous
programming, and user must supply their own async runtime. The async runtime
must also must include a scheduler to manage the tasks.\newline

Hopefully this provides enough context to why implementing a scheduler may be of
interest. In this chapter we will implement a very simple cooperative
scheduler.

\section{Design}

In this section we will define a spec for a simple task scheduler. The task sechdeuler has the following
requirements:
\begin{itemize}
    \item Supporting N execution context (ie. CPUs)
    \item Supporting T number of tasks
    \item Tasks have identical priority and are scheduled coopertively
    \item System has a single global lock
    \item Any task can attempt to acquire the lock, Any task attempting to
    acquire the lock are gauranteed to be scheduled.
    \item If multiple tasks attempt to grab the lock, the tasks will be
    scheduled in lock request order. 
\end{itemize}

\section{Spec}

We will model scheduler using the following variables:\newline
\begin{tla}
Init ==
    /\ cpus = [i \in 0..N-1 |-> ""] 
    /\ ready_q = SetToSeq(Tasks)
    /\ blocked_q = <<>>
    /\ lock_owner = ""
\end{tla}
\begin{tlatex}
\@x{ Init \.{\defeq}}%
 \@x{\@s{16.4} \.{\land} cpus \.{=} [ i \.{\in} 0 \.{\dotdot} N \.{-} 1
 \.{\mapsto}\@w{} ]}%
\@x{\@s{16.4} \.{\land} ready\_q \.{=} SetToSeq ( Tasks )}%
\@x{\@s{16.4} \.{\land} blocked\_q \.{=} {\langle} {\rangle}}%
\@x{\@s{16.4} \.{\land} lock\_owner \.{=}\@w{}}%
\end{tlatex}
\newline

A few things to note:
\begin{itemize}
    \item The system has \textit{N} executing context, represented as number of CPUs.
    When a task is running, \textit{cpus[k]} is set to \textit{taskName}. When CPU is idle,
    \textit{tcpus[k]} is set to an empty string. 
    \item \textit{ready\_q} and \textit{blocked\_q} are initialized as \textit{ordered tuple},
    due to the cooperative scheduling requirement.
    \item \textit{SetToSeq} is a macro from the community module \cite{tla_comm}
    to converts a set into a ordered tuple. To use community module, one can
    install required .tla files into the tla project source directly.
    \item Finally, the single system lock is represented as $lock\_owner$. 
\end{itemize}

A task can be in three possible state: \textit{Ready}, \textit{Blocked} and
\textit{Running}. The Spec includes required lock contention handling.\newline

\begin{tla}

Next == 
    \/ Running
    \/ Ready

MoveToReady(k) == 
    /\ cpus[k] # "" 
    /\ lock_owner # cpus[k]
    /\ ready_q' = Append(ready_q, cpus[k]) 
    /\ cpus' = [cpus EXCEPT ![k] = ""]
    /\ UNCHANGED <<lock_owner, blocked_q>>
Lock(k) == 
    \* lock is empty
    \/  /\ cpus[k] # "" 
        /\ lock_owner = ""
        /\ lock_owner' = cpus[k]
        /\ UNCHANGED <<ready_q, cpus, blocked_q>>
    \* someone else has the lock
    \/  /\ cpus[k] # "" 
        /\ lock_owner # ""
        /\ lock_owner # cpus[k] \* cannot double lock
        /\ blocked_q' = Append(blocked_q, cpus[k])
        /\ cpus' = [cpus EXCEPT ![k] = ""]
        /\ UNCHANGED <<ready_q, lock_owner>>
Unlock(k) == 
    /\ cpus[k] # "" 
    /\ lock_owner = cpus[k]
    /\ lock_owner' = ""
    /\ cpus' = [cpus EXCEPT ![k] = ""]
    /\ ready_q' = ready_q \o blocked_q \o <<cpus[k]>>
    /\ blocked_q' = <<>>

Running == 
    \E k \in DOMAIN cpus:
        /\ cpus[k] # "" 
        /\ \/ MoveToReady(k)
           \/ Lock(k)
           \/ Unlock(k)
\end{tla}
\begin{tlatex}
\@x{ MoveToReady ( k ) \.{\defeq}}%
\@x{\@s{16.4} \.{\land} cpus [ k ] \.{\neq}\@w{}}%
\@x{\@s{16.4} \.{\land} lock\_owner \.{\neq} cpus [ k ]}%
 \@x{\@s{16.4} \.{\land} ready\_q \.{'} \.{=} Append ( ready\_q ,\, cpus [ k ]
 )}%
 \@x{\@s{16.4} \.{\land} cpus \.{'} \.{=} [ cpus {\EXCEPT} {\bang} [ k ]
 \.{=}\@w{} ]}%
 \@x{\@s{16.4} \.{\land} {\UNCHANGED} {\langle} lock\_owner ,\, blocked\_q
 {\rangle}}%
\@x{ Lock ( k ) \.{\defeq}}%
\@x{}%
\@y{%
  lock is empty
}%
\@xx{}%
\@x{ \.{\lor}\@s{4.1} \.{\land} cpus [ k ] \.{\neq}\@w{}}%
\@x{\@s{4.1} \.{\land} lock\_owner \.{=}\@w{}}%
\@x{\@s{4.1} \.{\land} lock\_owner \.{'} \.{=} cpus [ k ]}%
 \@x{\@s{4.1} \.{\land} {\UNCHANGED} {\langle} ready\_q ,\, cpus ,\,
 blocked\_q {\rangle}}%
\@x{}%
\@y{%
  someone else has the lock
}%
\@xx{}%
\@x{ \.{\lor}\@s{4.1} \.{\land} cpus [ k ] \.{\neq}\@w{}}%
\@x{\@s{4.1} \.{\land} lock\_owner \.{\neq}\@w{}}%
\@x{\@s{4.1} \.{\land} lock\_owner \.{\neq} cpus [ k ]}%
\@y{%
  cannot double lock
}%
\@xx{}%
 \@x{\@s{4.1} \.{\land} blocked\_q \.{'} \.{=} Append ( blocked\_q ,\, cpus [
 k ] )}%
 \@x{\@s{4.1} \.{\land} cpus \.{'} \.{=} [ cpus {\EXCEPT} {\bang} [ k ]
 \.{=}\@w{} ]}%
 \@x{\@s{4.1} \.{\land} {\UNCHANGED} {\langle} ready\_q ,\, lock\_owner
 {\rangle}}%
\@x{ Unlock ( k ) \.{\defeq}}%
\@x{\@s{16.4} \.{\land} cpus [ k ] \.{\neq}\@w{}}%
\@x{\@s{16.4} \.{\land} lock\_owner \.{=} cpus [ k ]}%
\@x{\@s{16.4} \.{\land} lock\_owner \.{'} \.{=}\@w{}}%
 \@x{\@s{16.4} \.{\land} cpus \.{'} \.{=} [ cpus {\EXCEPT} {\bang} [ k ]
 \.{=}\@w{} ]}%
 \@x{\@s{16.4} \.{\land} ready\_q \.{'} \.{=} ready\_q \.{\circ} blocked\_q
 \.{\circ} {\langle} cpus [ k ] {\rangle}}%
\@x{\@s{16.4} \.{\land} blocked\_q \.{'} \.{=} {\langle} {\rangle}}%
\@pvspace{8.0pt}%
\@x{ Running \.{\defeq}}%
\@x{\@s{16.4} \E\, k \.{\in} {\DOMAIN} cpus \.{:}}%
\@x{\@s{20.5} \.{\land} cpus [ k ] \.{\neq}\@w{}}%
\@x{\@s{20.5} \.{\land} \.{\lor} MoveToReady ( k )}%
\@x{\@s{20.5} \.{\lor} Lock ( k )}%
\@x{\@s{20.5} \.{\lor} Unlock ( k )}%
\end{tlatex}

\section{Safety}

\section{Liveness}

I believe this is the most important part of cooperative scheduler design. While
the scheduler can't \textit{force} a task to relinquish a lock (the scheduler
doesn't dictate when the task is \textit{done}), the scheduler can ensure
scheduling fairness by scheduling the next lock requester intsead of the task 
that just relinquished the lock.\newline

\begin{tla}
----- MODULE scheduler ---- 
Liveness == 
    \A t \in Tasks:
        LET 
            b == {x \in DOMAIN blocked_q : blocked_q[x] = t}
        IN 
            /\ b # {} ~> b = {}
==========
\end{tla}
\begin{tlatex}
\@x{}\moduleLeftDash\@xx{ {\MODULE} scheduler}\moduleRightDash\@xx{}%
\@x{ Liveness \.{\defeq}}%
\@x{\@s{16.4} \A\, t \.{\in} Tasks \.{:}}%
\@x{\@s{20.5} \.{\LET}}%
 \@x{\@s{36.9} b \.{\defeq} \{ x \.{\in} {\DOMAIN} blocked\_q \.{:} blocked\_q
 [ x ] \.{=} t \}}%
\@x{\@s{20.5} \.{\IN}}%
\@x{\@s{36.9} \.{\land} b \.{\neq} \{ \} \.{\leadsto} b \.{=} \{ \}}%
\@x{}\bottombar\@xx{}%
\end{tlatex}
\newline

The formula defines set b to be either an empty set or a set of one task. Assume
a set of \{"p0", "p1", "p2\}. Possible value of b include: \{\}, \{"p0"\},
\{"p1"\} and \{"p2"\}. The fomula then states an non empty set of b leads to an
\textit{empty set} of b. In other words: \newline

\textit{If a task ever becomes blocked, it will eventually become unblocked}.\newline

However, when we actually run the model checker, we will find the liveness
property is \textit{violated}. The failure scenario is basically one task
holding onto the lock in one CPU, while the scheduler repeatedly
schedule/deschedule a separate task in another CPU. While this is perfectly
allowed, the model checker detects a possible path for the the system to trap in
a local state and fail the liveness property.\newline 

Perhaps not surprisingly, if you construct similar liveness property to verify 
a task is \textit{eventually} always scheduled, it will also fail. The model
checker will provide a counter case where a task is never scheduled because
another task is repeatedly acquire/release the global lock.\newline

We need \textit{Strong Fairness} to solve this problem:
\begin{tla}
----- MODULE scheduler ---- 
L ==
    \A t \in Tasks :
        \A n \in 0..(N-1) :
            WF_vars(HoldingLock(t) /\ Unlock(n))
Spec ==
  /\ Init
  /\ [][Next]_vars
  /\ WF_vars(Next)
  /\ L 
======= 
\end{tla}
\begin{tlatex}
\@x{}\moduleLeftDash\@xx{ {\MODULE} scheduler}\moduleRightDash\@xx{}%
\@x{ L \.{\defeq}}%
\@x{\@s{8.2} \A\, t \.{\in} Tasks \.{:}}%
\@x{\@s{12.29} \A\, n \.{\in} 0 \.{\dotdot} ( N \.{-} 1 ) \.{:}}%
\@x{\@s{16.4} {\WF}_{ vars} ( HoldingLock ( t ) \.{\land} Unlock ( n ) )}%
\@x{ Spec \.{\defeq}}%
\@x{\@s{8.2} \.{\land} Init}%
\@x{\@s{8.2} \.{\land} {\Box} [ Next ]_{ vars}}%
\@x{\@s{8.2} \.{\land} {\WF}_{ vars} ( Next )}%
\@x{\@s{8.2} \.{\land} L}%
\@x{}\bottombar\@xx{}%
\end{tlatex}
\newline

Fairness ensures that we are never stuck in a repeated state.

% \end{document}
